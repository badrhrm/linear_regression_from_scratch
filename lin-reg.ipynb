{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ba3ae6a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-31T01:23:41.894356Z",
     "iopub.status.busy": "2026-01-31T01:23:41.894031Z",
     "iopub.status.idle": "2026-01-31T01:23:45.526516Z",
     "shell.execute_reply": "2026-01-31T01:23:45.525650Z"
    },
    "papermill": {
     "duration": 3.638045,
     "end_time": "2026-01-31T01:23:45.528470",
     "exception": false,
     "start_time": "2026-01-31T01:23:41.890425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (442, 10)\n",
      "Target shape:   (442,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Load the dataset\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "# 2. Check the shapes (Crucial Step!)\n",
    "print(f\"Features shape: {X.shape}\")  # Should be (442, 10) -> 442 samples, 10 features\n",
    "print(f\"Target shape:   {y.shape}\")  # Should be (442,) -> A flat vector\n",
    "\n",
    "# 3. Split into Train and Test\n",
    "# We train on 80% of the data, test on 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08644e09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T01:23:45.533629Z",
     "iopub.status.busy": "2026-01-31T01:23:45.532832Z",
     "iopub.status.idle": "2026-01-31T01:23:45.693216Z",
     "shell.execute_reply": "2026-01-31T01:23:45.692328Z"
    },
    "papermill": {
     "duration": 0.164936,
     "end_time": "2026-01-31T01:23:45.695091",
     "exception": false,
     "start_time": "2026-01-31T01:23:45.530155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged at iteration 6509\n",
      "final training mse_vector: 2898.1708163973576\n",
      "final training cost_diff: 0.000999435941594129\n",
      "final test mse 2879.872544907411\n",
      "r^2 score 0.4564382672289956\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def linreg(features_matrix,y, X_test, y_test) :\n",
    "    y_vector = y.reshape(-1,1)\n",
    "    # we get number of features we have in total\n",
    "    nbr_features = features_matrix.shape[1]\n",
    "    \n",
    "    weights_vector = np.zeros(nbr_features).reshape(-1,1)\n",
    "    intercept_scalar = 0\n",
    "    \n",
    "    #print(\"Shape of X:\", features_matrix.shape)\n",
    "    #print(\"Shape of y:\", y_vector.shape)\n",
    "    #print(\"Shape of w:\", weights_vector.shape) \n",
    "    \n",
    "    # rows,cols . cols,1 = rows,\n",
    "    y_pred_vector = np.dot(features_matrix, weights_vector) + intercept_scalar\n",
    "    \n",
    "    #print(\"Shape of y_pred:\", y_pred_vector.shape)\n",
    "    \n",
    "    error_vector = y_pred_vector - y_vector\n",
    "    #print(\"error shape:\", error_vector.shape)\n",
    "    m_number_of_examples = features_matrix.shape[0]\n",
    "    mse_vector = np.sum(error_vector**2)/m_number_of_examples\n",
    "\n",
    "    dj_dy_pred = 1/m_number_of_examples * error_vector # y_pred_vector - y\n",
    "    dy_pred_dw = features_matrix #X\n",
    "    dy_pred_db = 1\n",
    "    gd_iters = 0\n",
    "    epsilon = 0.001\n",
    "    alpha = 0.3\n",
    "\n",
    "    #previous_params[0] is weights_vector   while  previous_params[1] is intercept_scalar\n",
    "    previous_params = None\n",
    "\n",
    "    \n",
    "    while (True) : \n",
    "        # cols,rows  .  rows,1 = cols,1 \n",
    "        dj_dw = np.dot(dy_pred_dw.T, dj_dy_pred) # chain rule\n",
    "        #print(\"######\")\n",
    "        #print(\"weights_vector1:\",weights_vector.shape)\n",
    "        weights_vector = weights_vector - (alpha * dj_dw)\n",
    "        #print(\"dj_dw\",dj_dw.shape)\n",
    "        #print(\"weights_vector2:\",weights_vector.shape)\n",
    "        #print(\"######\")\n",
    "\n",
    "        # summing the error vector into a scaler explicitly \n",
    "        # unlike in dj_dw it was  not needed cause dot product does it implicitly\n",
    "        dj_db = np.sum(dj_dy_pred) * dy_pred_db # chain rule\n",
    "        #print(\"######\")\n",
    "        #print(\"dj_db\", dj_db.shape)\n",
    "        intercept_scalar = intercept_scalar - (alpha * dj_db)\n",
    "        #print(\"intercept_scalar\", intercept_scalar.shape)\n",
    "        #print(\"######\")\n",
    "        \n",
    "        y_pred_vector = np.dot(features_matrix, weights_vector) + intercept_scalar\n",
    "        error_vector = y_pred_vector - y_vector\n",
    "        dj_dy_pred = 1/m_number_of_examples * error_vector\n",
    "        previous_mse_vector = mse_vector \n",
    "        mse_vector = np.sum(error_vector**2)/m_number_of_examples\n",
    "        \n",
    "    \n",
    "        # delta loss \n",
    "        cost_diff = previous_mse_vector - mse_vector \n",
    "        #print(\"cost_diff:\", cost_diff)\n",
    "        \n",
    "        if cost_diff < 0:  # current_cost > prev_cost\n",
    "            # revert back to previous params\n",
    "            print(\"current weights_vector:\",weights_vector.T)\n",
    "            print(\"current intercept_scalar:\",intercept_scalar)\n",
    "            weights_vector = previous_params[0]\n",
    "            intercept_scalar = previous_params[1]\n",
    "            print(\"previous weights_vector:\",weights_vector.T)\n",
    "            print(\"previous intercept_scalar:\",intercept_scalar)\n",
    "            print(f\"WARNING: cost increased from [previous_cost: {previous_mse_vector}] to [current_cost: {mse_vector}] at iteration [{gd_iters}]\")\n",
    "            # we should ecrease learning rate alpha\n",
    "            break\n",
    "        if ( cost_diff < epsilon ): \n",
    "            print(f\"converged at iteration {gd_iters}\")\n",
    "            print(\"final training mse_vector:\", mse_vector)\n",
    "            print(\"final training cost_diff:\", cost_diff)\n",
    "            break \n",
    "\n",
    "        previous_params = [weights_vector, intercept_scalar]\n",
    "        if gd_iters >= 10000 : \n",
    "            print(\"Gradient Descent reach max iterations [10000]\")\n",
    "            print(\"final training mse_vector:\", mse_vector)\n",
    "            print(\"final training cost_diff:\", cost_diff)\n",
    "            break\n",
    "        gd_iters+=1\n",
    "\n",
    "    y_test = y_test.reshape(-1,1)\n",
    "    \n",
    "    final_y_pred = (X_test @ weights_vector) + intercept_scalar \n",
    "    final_mse =  1/X_test.shape[0] * np.sum((final_y_pred - y_test)**2) \n",
    "    print(\"final test mse\",final_mse)\n",
    "    print(\"r^2 score\", 1 - ( np.sum((y_test - final_y_pred)**2) / np.sum((y_test - y_test.mean())**2) ))\n",
    "\n",
    "linreg(X_train, y_train,X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4d980ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T01:23:45.699929Z",
     "iopub.status.busy": "2026-01-31T01:23:45.699331Z",
     "iopub.status.idle": "2026-01-31T01:23:45.840971Z",
     "shell.execute_reply": "2026-01-31T01:23:45.840060Z"
    },
    "papermill": {
     "duration": 0.146148,
     "end_time": "2026-01-31T01:23:45.842863",
     "exception": false,
     "start_time": "2026-01-31T01:23:45.696715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4526027629719195"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708953e5",
   "metadata": {
    "papermill": {
     "duration": 0.00155,
     "end_time": "2026-01-31T01:23:45.846163",
     "exception": false,
     "start_time": "2026-01-31T01:23:45.844613",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# todos:\n",
    "- I need to use OOP\n",
    "- I need to try closed form solution for minima \n",
    "- train_test_split without using sklearn\n",
    "- I need to add SGD and mini batch GD \n",
    "- I need to store mse in a list so i can plot learning curve \n",
    "- I need to implement poly\n",
    "- I need to implement Regularization (Ridge/Lasso)\n",
    "- GD with optimal step ?"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.005805,
   "end_time": "2026-01-31T01:23:46.466996",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-31T01:23:38.461191",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
